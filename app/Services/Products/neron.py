import numpy as np
import random as rd
import random
import sys

INPUT_DIM = 4
OUT_DIM = 3
H_DIM = 10

def relu(t):
    return np.maximum(t, 0)

def softmax(t):
    out = np.exp(t)
    return out / np.sum(out)

def softmax_batch(t):
    out = np.exp(t)
    return out / np.sum(out, axis=1, keepdims=True)

def sparse_cross_entropy(z, y):
    return -np.log(z[0, y])

def sparse_cross_entropy_batch(z, y):
    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))

def to_full(y, num_classes):
    y_full = np.zeros((1, num_classes))
    y_full[0, y] = 1
    return y_full

def to_full_batch(y, num_classes):
    y_full = np.zeros((len(y), num_classes))
    for j, yj in enumerate(y):
        y_full[j, yj] = 1
    return y_full

def relu_deriv(t):
    return (t >= 0).astype(float)

def datasetInfo():
    return [
        ([[42, 10.5, 1.4, 8.2]], 0), 
        ([[40.9, 12. , 1.4, 8.2]], 0), 
        ([[40.7, 10.2, 1.3, 8.2]], 0), 
        ([[35.6, 10.1, 1.5, 9.2]], 0), 
        ([[40. , 10.6, 1.4, 8.2]], 0), 
        ([[40.4, 10.9, 1.7, 8.4]], 0), 
        ([[40.6, 10.4, 1.4, 8.3]], 0), 
        ([[40. , 10.4, 1.5, 7.2]], 0), 
        ([[48.4, 10.9, 1.4, 7.2]], 0), 
        ([[42.9, 10.1, 1.5, 7.1]], 0), 
        ([[45.4, 10.7, 1.5, 9.2]], 0), 
        ([[40.8, 10.4, 1.6, 8.2]], 0), 
        ([[40.8, 10. , 1.4, 8.1]], 0), 
        ([[45.3, 10. , 1.1, 9.1]], 0), 
        ([[40.8, 10. , 1.2, 7.2]], 0), 
        ([[35.7, 10.4, 1.5, 8.4]], 0), 
        ([[45.4, 10.9, 1.3, 8.4]], 0), 
        ([[35.1, 10.5, 1.4, 8.3]], 0), 
        ([[45.7, 10.8, 1.7, 8.3]], 0), 
        ([[35.1, 10.8, 1.5, 7.3]], 0), 
        ([[48.4, 10.4, 1.7, 9.2]], 0), 
        ([[41.1, 10.7, 1.5, 7.4]], 0), 
        ([[0.5, 0.6, 1. , 9.2]], 0), 
        ([[0.5, 0.5, 1.7, 9.5]], 0), 
        ([[0.8, 0.8, 1.9, 7.2]], 0), 
        ([[40. , 10. , 1.6, 8.2]], 0), 
        ([[40. , 10.4, 1.6, 7.4]], 0), 
        ([[35.2, 10.5, 1.5, 9.2]], 0), 
        ([[34.2, 10.4, 1.4, 7.2]], 0), 
        ([[34.7, 10.2, 1.6, 9.2]], 0), 
        ([[40.8, 10.1, 1.6, 7.2]], 0), 
        ([[45.4, 10.4, 1.5, 8.4]], 0), 
        ([[45.2, 10.1, 1.5, 9.1]], 0), 
        ([[35.5, 10.2, 1.4, 7.2]], 0), 
        ([[42.9, 10.1, 1.5, 8.2]], 0), 
        ([[45. , 10.2, 1.2, 9.2]], 0), 
        ([[45.5, 10.5, 1.3, 8.2]], 0), 
        ([[44.9, 10.6, 1.4, 9.1]], 0), 
        ([[44.4, 10. , 1.3, 8.2]], 0), 
        ([[45.1, 10.4, 1.5, 9.2]], 0),
        ([[35. , 10.5, 1.3, 9.3]], 0), 
        ([[39.5, 10.3, 1.3, 7.3]], 0), 
        ([[34.4, 10.2, 1.3, 7.2]], 0), 
        ([[35. , 10.5, 1.6, 8.6]], 0), 
        ([[35.1, 10.8, 1.9, 8.4]], 0), 
        ([[34.8, 10. , 1.4, 9.3]], 0), 
        ([[45.1, 10.8, 1.6, 9.2]], 0), 
        ([[44.6, 10.2, 1.4, 9.2]], 0), 
        ([[25.3, 10.7, 1.5, 9.2]], 0), 
        ([[45. , 10.3, 1.4, 9.2]], 0), #coco cola
        ([[44.2 , 9, 4.7, 1.4]], 1), #jucie
        ([[43.8, 9.2, 4.5, 0.1]], 1), 
        ([[47.1, 9.1, 4.9, 0.1]], 1), 
        ([[44.5, 9.3, 4. , 0.3]], 1), 
        ([[46.3, 9.8, 4.6, 0.5]], 1), 
        ([[42.9, 9.8, 4.5, 0.3]], 1), 
        ([[48.2, 9.3, 4.7, 0.6]], 1), 
        ([[45.6, 9.4, 3.3, 0. ]], 1), 
        ([[44.1, 9.9, 4.6, 0.3]], 1), 
        ([[46.8, 9.7, 3.9, 0.4]], 1), 
        ([[45. , 9. , 3.5, 0. ]], 1), 
        ([[45.9, 9. , 4.2, 0.5]], 1), 
        ([[46. , 9.2, 4. , 0. ]], 1), 
        ([[46.1, 9.9, 4.7, 0.4]], 1), 
        ([[45.6, 9.9, 3.6, 0.3]], 1), 
        ([[46.7, 9.1, 4.4, 0.4]], 1), 
        ([[45.6, 9. , 4.5, 0.5]], 1), 
        ([[45.8, 8.7, 4.1, 0. ]], 1), 
        ([[46.2, 8.2, 4.5, 0.5]], 1), 
        ([[45.6, 8.5, 3.9, 0.1]], 1), 
        ([[45.9, 9.2, 4.8, 0.8]], 1),
        ([[46.1, 8.8, 4. , 0.3]], 1), 
        ([[46.3, 8.5, 4.9, 0.5]], 1), 
        ([[46.1, 8.8, 4.7, 0.2]], 1), 
        ([[46.4, 8.9, 4.3, .3]], 1), 
        ([[46.6, 9. , 4.4, 0.4]], 1), 
        ([[46.8, 8.8, 4.8, 0.4]], 1), 
        ([[46.7, 9. , 5. , 0.7]], 1), 
        ([[46. , 8.9, 4.5, 0.5]], 1),
        ([[45.7, 8.6, 3.5, 0. ]], 1),
        ([[45.5, 9.4, 3.8, 0.1]], 1),
        ([[45.5, 9.4, 3.7, 0. ]], 1),
        ([[45.8, 9.7, 3.9, 0.2]], 1),
        ([[46. , 9.7, 5.1, 0.6]], 1),
        ([[45.4, 9. , 4.5, 0.5]], 1),
        ([[46. , 9.4, 4.5, 0.6]], 1),
        ([[46.7, 9.1, 4.7, 0.5]], 1),
        ([[46.3, 9.3, 4.4, 0.3]], 1),
        ([[45.6, 9. , 4.1, 0.3]], 1),
        ([[45.5, 9.5, 4. , 0.3]], 1),
        ([[45.5, 9. , 4.4, 0.2]], 1),
        ([[46.1, 9. , 4.6, 0.4]], 1),
        ([[45.8, 9.6, 4. , 0.2]], 1), 
        ([[45. , 8.3, 3.3, 0. ]], 1),
        ([[45.6, 8.7, 4.2, 0.3]], 1), 
        ([[45.7, 8. , 4.2, 0.2]], 1), 
        ([[45.7, 8.9, 4.2, 0.3]], 1), 
        ([[46.2, 8.9, 4.3, 0.3]], 1),
        ([[120. , 13. , 5.8, 2.2]], 2), #wine 
        ([[123.6, 11. , 6.6, 2.1]], 2), 
        ([[140.9, 14.5, 4.5, 1.7]], 2), 
        ([[127.3, 11.9, 6.3, 1.8]], 2), 
        ([[50.7, 4.5, 5.8, 1.8]], 2), 
        ([[79.2, 8.6, 6.1, 2.5]], 2), 
        ([[140.5, 12.2, 5.1, 2. ]], 2), 
        ([[76.4, 7.7, 5.3, 1.9]], 2), 
        ([[56.8, 4. , 5.5, 2.1]], 2), 
        ([[65.7, 6.5, 5. , 2. ]], 2),
        ([[45.8, 4.8, 5.1, 2.4]], 2), 
        ([[116.4, 3.2, 5.3, 2.3]], 2), 
        ([[96.5, 5. , 5.5, 1.8]], 2), 
        ([[97.7, 3.8, 6.7, 2.2]], 2), 
        ([[87.7, 4.6, 6.9, 2.3]], 2), 
        ([[76. , 2.2, 5. , 1.5]], 2), 
        ([[66.9, 5.2, 5.7, 2.3]], 2), 
        ([[65.6, 6.8, 4.9, 2. ]], 2), 
        ([[67.7, 5.8, 6.7, 2. ]], 2), 
        ([[66.3, 3.7, 4.9, 1.8]], 2),
        ([[66.7, 5.3, 5.7, 2.1]], 2), 
        ([[67.2, 5.2, 6. , 1.8]], 2),
        ([[66.2, 6.8, 4.8, 1.8]], 2), 
        ([[66.1, 4. , 4.9, 1.8]], 2),
        ([[66.4, 3.8, 5.6, 2.1]], 2), 
        ([[67.2, 4. , 5.8, 1.6]], 2),
        ([[77.4, 7.8, 6.1, 1.9]], 2),
        ([[77.9, 4.8, 6.4, 2. ]], 2),
        ([[76.4, 7.8, 5.6, 2.2]], 2), 
        ([[76.3, 7.8, 5.1, 1.5]], 2),
        ([[76.1, 7.6, 5.6, 1.4]], 2),
        ([[77.7, 7. , 6.1, 2.3]], 2), 
        ([[76.3, 7.4, 5.6, 2.4]], 2), 
        ([[106.4, 10.1, 5.5, 1.8]], 2), 
        ([[106. , 10. , 4.8, 1.8]], 2), 
        ([[106.9, 10.1, 5.4, 2.1]], 2),
        ([[126.7, 11.1, 5.6, 2.4]], 2), 
        ([[136.9, 12.1, 5.1, 2.3]], 2), 
        ([[125.8, 12.7, 5.1, 1.9]], 2), 
        ([[146.8, 13.2, 5.9, 2.3]], 2), 
        ([[126.7, 12.3, 5.7, 2.5]], 2), 
        ([[146.7, 14. , 5.2, 2.3]], 2), 
        ([[136.3, 11.5, 5. , 1.9]], 2),
        ([[126.5, 12. , 5.2, 2. ]], 2),
        ([[126.2, 12.4, 5.4, 2.3]], 2),
        ([[105.9, 10. , 5.1, 1.8]], 2)]   
   
    
from sklearn import datasets
dataset = datasetInfo()

W1 = np.random.rand(INPUT_DIM, H_DIM)
b1 = np.random.rand(1, H_DIM)
W2 = np.random.rand(H_DIM, OUT_DIM)
b2 = np.random.rand(1, OUT_DIM)

W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)
b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)
W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)
b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)

ALPHA = 0.0002
NUM_EPOCHS = 400
BATCH_SIZE = 50

loss_arr = []

for ep in range(NUM_EPOCHS):
    random.shuffle(dataset)
    for i in range(len(dataset) // BATCH_SIZE):

        batch_x, batch_y = zip(*dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])
        x = np.concatenate(batch_x, axis=0)
        y = np.array(batch_y)

        # Forward
        t1 = x @ W1 + b1
        h1 = relu(t1)
        t2 = h1 @ W2 + b2
        z = softmax_batch(t2)
        E = np.sum(sparse_cross_entropy_batch(z, y))

        # Backward
        y_full = to_full_batch(y, OUT_DIM)
        dE_dt2 = z - y_full
        dE_dW2 = h1.T @ dE_dt2
        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)
        dE_dh1 = dE_dt2 @ W2.T
        dE_dt1 = dE_dh1 * relu_deriv(t1)
        dE_dW1 = x.T @ dE_dt1
        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)

        # Update
        W1 = W1 - ALPHA * dE_dW1
        b1 = b1 - ALPHA * dE_db1
        W2 = W2 - ALPHA * dE_dW2
        b2 = b2 - ALPHA * dE_db2

        loss_arr.append(E)

def predict(x):
    t1 = x @ W1 + b1
    h1 = relu(t1)
    t2 = h1 @ W2 + b2
    z = softmax_batch(t2)
    return z

def calc_accuracy():
    correct = 0
    for x, y in dataset:
        z = predict(x)
        y_pred = np.argmax(z)
        if y_pred == y:
            correct += 1
    acc = correct / len(dataset)
    return acc

accuracy = calc_accuracy()

# print("Accuracy:", accuracy)

sugar = float(sys.argv[1])
colories = float(sys.argv[2])
color = float(sys.argv[3])
gas = float(sys.argv[4])
test = [colories, sugar , color, gas]
x = np.array(test)


def relu(t):
    print('relu:1', np.maximum(t, 0))
    return np.maximum(t, 0)

def softmax(t):
    out = np.exp(t)
    print('softmax:', out / np.sum(out))
    return out / np.sum(out)

def predict(x):
    t1 = x @ W1 + b1
    h1 = relu(t1)
    t2 = h1 @ W2 + b2
    z = softmax(t2)
    print('z =1', z)
    return z

tl = x @ W1 + b1
hl = relu(tl)

probs = predict(x)
pred_class = np.argmax(probs)

# class_names = ['Сок', 'Лимонад', 'Вино']
class_names = ['Лимонад','Сок', 'Вино']
# print('Predicted:', class_names[pred_class])

print(pred_class+1)